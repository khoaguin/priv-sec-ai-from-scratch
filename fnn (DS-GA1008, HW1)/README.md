# Fully Connected Neural Network
This implementation is my solutions to the first homework of the course Deep Learning at NYU, 2021 from the legendary Yann Lecun and Alfredo Canziani. [Here](https://atcold.github.io/NYU-DLSP21/) is the link to the course. In this homework, we will implement a simple neural network with 2 linear layers (both forward and backward pass). Please use this responsibly according to the honor code of the course, as we do not want to make the teachers implement a new homework every year.

Of course, understanding the theory is very important before we even write any code. `theory.pdf` contains my answers for the theory part in `homework1.pdf`. I try to make things as clear as possible, however, there are parts that I still have doubts. It is great if you can walk through them and find out my mistakes. Please make an issue to notify me about any mistakes, I am more than happy to discuss and fix them.  

The implementation part is in `mlp.py`. Based on the coure's homework, torch `Tensor` is used for convenience. Torch tensors are similar to numpy arrays, but tensors can be oprated on CUDA GPUs, and they can also keep the gradients of the loss function w.r.t themselves for automatic backward. Learn more about the differences between numpy arrays and torch tensors [here](https://medium.com/@ashish.iitr2015/comparison-between-pytorch-tensor-and-numpy-array-de41e389c213). PyTorch offers `autograd` functionality where we can define the forward pass to calculate the scalar tensor `loss`, then use `loss.backward()` to command the library to calculate the gradients for us. However, in this implementation, we will not use the torch's `autograd` but implement the backward pass ourselves.  
I also added new tests:
- `test_forward_mse.py`: compares the values of `y_hat`, `mse_loss`, and the derivatives of `mse_loss` with respect to `y_hat` of the forward pass of the manual neural net in `mlp.py` to the outputs of the same network using `torch`.
- `test_forward_bce.py`: compares the values of `y_hat`, `bce_loss`, and the derivatives of `bce_loss` with respect to `y_hat` of the forward pass of the manual neural net in `mlp.py` to the outputs of the same network using `torch`. There is one problem when using `sigmoid` and `bce_loss`: when you increase the number of features in the linear layers, the results may become wrong as there are divisions by very small numbers. This is something that I need to investigate deeper.
